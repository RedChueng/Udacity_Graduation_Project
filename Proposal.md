# 机器学习工程师纳米学位

## 毕业项目开题报告

张红亮 Udacity
2018年7月17日
## 猫狗大战

### 项目背景

猫狗分类问题是计算机视觉领域的经典问题，识别图片中的猫和狗对人类来说，2岁小孩即可轻松完成，但是让计算机完成这一任务，却是曾经机器学习技术难以攻克的一座大山，直到30年前，深度学习之父杰弗里·辛顿将多层神经网络带入机器学习领域，它为近10年来深度学习的发展奠定了基础，使得这个曾经困扰很多机器学习领域实践者多年的问题迎刃而解，近年来也涌现出非常多的用于图像识别的深度学习模型，计算机视觉成为人工智能研究的热门领域。

大数据竞赛平台Kaggle提供了一个供机器学习爱好者自我实践的竞赛项目[《Cats vs. Dogs》](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)，在这个竞赛中，Kaggle提供了25000张猫和狗的图片作为训练数据集，提供了12500张猫和狗的图片作为测试集，本项目将基于此数据集完成。

### 问题描述

该项目需要解决的问题：输入一张彩色图片，输出狗的概率。

此问题是计算机视觉领域一个典型的图像识别问题，而且是一个监督学习的二分类问题，应用深度学习技术可以获得较好的识别准确率，在近10年来有很多相关的表现优秀的深度学习模型的诞生，特别是受生物视觉原理启发的[卷积神经网络CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network)成为解决此类问题的首选模型，本项目也将采用CNN模型。

### 输入数据

用于模型训练和测试的数据集由[Kaggle](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data)提供，数据集下载后是一个压缩文件，解压后可以得到训练集和测试集文件夹。训练集文件夹中包含25000张猫狗彩色图片，其中猫和狗各12500张，文件以猫狗标签以及文件编号命名；测试集文件夹中包含打乱的12500张猫狗图片，猫和狗随机分布，文件以编号命名。模型训练的目标就是以较高的准确率给出测试集中每个图片是狗的概率。

![数据集示例](./img/dataset.png)

经过仔细的观察，我发现训练集的数据有以下几个特点：
* 猫和狗的图像文件是按顺序排列的，前12500个文件是猫，后12500个文件是狗，因此在数据预处理时需要做乱序处理；
* 图像文件的宽高不一致，需要对图片做缩放处理，统一宽高；
* 有一些图像文件清晰度不够，光照条件不足，图像上的内容不光只有猫和狗，背景比较复杂，有一些是对猫和狗的日常进行的抓拍，猫和狗的脸都看不见，还有一些图片甚至根本没有猫和狗，这些都是异常数据，因此在训练模型时要防止过拟合。

### 解决办法

本项目的主要过程是利用训练数据集分批次对项目指导提供的一些知名的CNN模型进行训练，根据定义好的模型评价指标，做模型做评价验证，选出表现最佳的模型。基准模型理论上不能达到kaggle竞赛前10%的要求的，需要对模型进行适当调整使之更符合数据集特征，直到模型表现符合要求，然后用调参后的模型在测试数据集上测试。本项目将使用Keras完成，后端引擎使用TensorFlow，由于计算量较大，将使用亚马逊p3.2xlarge云服务器来完成。

### 基准模型

* **Xception**
  - Xception发布于2016年，由François Chollet提出，它的假设是跨通道的相关性和空间相关性是完全可分离的，最好不要联合映射它们，在传统的卷积网络中，卷积层会同时寻找跨空间和跨深度的相关性，Xception不同时寻找二者的相关性[1]；
  - Xception首先求一个2D空间的相关性，再通过后面跟一个跨通道的1x1卷积，求一个1D空间的相关性，这种映射学起来比全3D映射更加简单，事实证明，它的计算效率也更高，特别是在大规模数据上，表现突出[1]。

本项目的最低要求是kaggle竞赛[Dogs vs. Cats Redux: Kernels Edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/leaderboard) public leaderboard前10%，目前该竞赛的参赛人数是1314，也就是最低需要达到131名，目前131名选手的得分是0.06127，基准模型不能满足项目要求，需要手动调参。

### 评估指标

根据kaggle竞赛的要求，本项目将采用对数损失函数来评价模型表现：

$$
\textrm{LogLoss} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\right],
$$
其中，

* $ n $ 为测试集中图片的数量；
* $ \hat {y}_i​$ 为预测图片内容为狗的概率；
* $ y_i $ 如果图片内容是狗，则为1，如果是猫，则为0；
* $ \log() $ 为自然（底为$e$）对数。

logloss值越小，模型的表现越好。

### 设计大纲

1. 利用kaggle API下载数据集，因为下载的是zip文件，需要用wget解压；
2. 正确读取训练数据后，对训练数据进行可视化，探索数据特征；
3. 进行数据预处理，对训练集数据进行压缩、规则化、独热编码和乱序处理；
4. 采用keras预置的Xception模型作为基准模型；
5. 根据本项目的数据特点，对基准模型进行调整，比如将Xception的输出层改为二分类的全连接；
6. 利用训练数据对模型进行训练，可视化展示模型的logloss值以及accuracy值；
7. 评估模型，如果模型变现不好，返回步骤5；
8. 利用测试数据集对模型进行测试，得到测试集logloss值和accuracy值，并随机可视化展示几个测试集中的测试结果。

### 参考文献
[1] Chollet F. Xception: Deep Learning with Depthwise Separable Convolutions[J]. 2016:1800-1807.
